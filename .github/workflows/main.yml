name: preprocessing

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    
permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # Checkout source code
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      # Set up Python environment
      - name: Set up Python 3.12.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.12.11"

      # Install dependencies (incl. mlflow)
      - name: Set up Miniconda
        uses: conda-incubator/setup-miniconda@v2
        with:
          auto-activate-base: false
          python-version: 3.12.11
          
      # Install dependency
      - name: Install dependencies with pip inside Conda env
        shell: bash -l {0}
        run: |
          conda create -y -n smsml python=3.12.11
          conda activate smsml
          pip install --upgrade pip
          pip install -r preprocessing/requirements.txt
          python -m spacy download en_core_web_sm
          mkdir dataset

      # Run preprocessing python file
      - name: Run training script
        shell : bash -l {0}
        run: |
          conda activate smsml
          python preprocessing/automate_Tri_Nuryatul.py
          
      # move file result to processing
      - name: Move dataset to preprocessing dataset folder
        run: |
          pwd
          ls
          mv ./tokenizer.pkl preprocessing/tokenizer.pkl
          mv ./automate_ner_dataset_ML.pkl preprocessing/dataset/automate_ner_dataset_ML.pkl
          mv ./automate_ner_dataset_DL.pkl preprocessing/dataset/automate_ner_dataset_DL.pkl
          ls preprocessing
          
      # Commit & Push Updated Files
      - name: Save data split and tokenizer to repo
        run: |
          cd preprocessing
          git config --global user.name ${{ secrets.username }}
          git config --global user.email ${{ secrets.email }}
          git status
          git add tokenizer.pkl dataset/
          git commit -m "Auto-update dataset and tokenizer via GitHub Actions"
          git push origin main
  
